"""
Nitter å®ä¾‹å¥åº·æ£€æµ‹æ¨¡å—
æ£€æµ‹å®ä¾‹çš„å¯ç”¨æ€§å’Œå“åº”æ—¶é—´ï¼Œå¹¶ä½¿ç”¨ Redis ç¼“å­˜ç»“æœ
"""

import json
import time
import logging
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests

from src.utils.logger import setup_logger
from src.crawler.instance_sources import get_default_sources
from src.crawler.constants import KNOWN_INSTANCES
from src.storage.redis_client import get_redis_client
from src.config.redis_keys import REDIS_KEY_AVAILABLE_INSTANCES, CACHE_EXPIRE_INSTANCES

logger = setup_logger("nitter_discovery", log_file="logs/nitter_discovery.log")


class NitterInstanceChecker:
    """Nitter å®ä¾‹å¥åº·æ£€æµ‹å™¨"""

    def __init__(self, timeout: int = 10, max_workers: int = 20):
        """
        åˆå§‹åŒ–å¥åº·æ£€æµ‹å™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: å¹¶å‘æ£€æµ‹çº¿ç¨‹æ•°
        """
        self.timeout = timeout
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "DNT": "1",
        })

    def check_instance(self, instance_url: str) -> Tuple[str, bool, float]:
        """
        æ£€æŸ¥å•ä¸ªå®ä¾‹çš„å¥åº·çŠ¶æ€

        Args:
            instance_url: å®ä¾‹ URL

        Returns:
            (URL, æ˜¯å¦å¯ç”¨, å“åº”æ—¶é—´)
        """
        try:
            start_time = time.time()

            # è®¿é—®å®ä¾‹é¦–é¡µ
            response = self.session.get(
                instance_url,
                timeout=self.timeout,
                allow_redirects=True
            )

            elapsed = time.time() - start_time

            # åˆ¤æ–­æ˜¯å¦å¯ç”¨
            if response.status_code == 200:
                # æ£€æŸ¥æ˜¯å¦åŒ…å« Nitter ç‰¹å¾ï¼ˆæ›´ä¸¥æ ¼çš„åˆ¤æ–­ï¼‰
                content = response.text.lower()

                # æ’é™¤æ˜æ˜¾ä¸æ˜¯ Nitter çš„ç½‘ç«™
                if "github" in instance_url.lower():
                    return (instance_url, False, elapsed)

                # æ£€æŸ¥ Nitter ç‰¹å¾
                nitter_indicators = ["nitter", "instance", "bird", "unofficial"]
                has_indicator = any(keyword in content for keyword in nitter_indicators)

                # æˆ–è€… URL ä¸­åŒ…å« nitter ç›¸å…³å…³é”®è¯
                has_nitter_url = any(
                    keyword in instance_url.lower()
                    for keyword in ["nitter", "bird", "twitter", "xcancel"]
                )

                is_nitter = has_indicator or has_nitter_url

                if is_nitter:
                    logger.info(f"âœ“ {instance_url} å¯ç”¨ (å“åº”æ—¶é—´: {elapsed:.2f}s)")
                    return (instance_url, True, elapsed)

            logger.debug(f"âœ— {instance_url} ä¸å¯ç”¨ (çŠ¶æ€ç : {response.status_code})")
            return (instance_url, False, elapsed)

        except requests.exceptions.Timeout:
            logger.debug(f"âœ— {instance_url} è¶…æ—¶")
            return (instance_url, False, self.timeout)

        except Exception as e:
            logger.debug(f"âœ— {instance_url} é”™è¯¯: {e}")
            return (instance_url, False, 0)

    def check_instances_batch(self, instances: List[str]) -> List[Dict[str, any]]:
        """
        å¹¶å‘æ£€æµ‹å¤šä¸ªå®ä¾‹

        Args:
            instances: å®ä¾‹ URL åˆ—è¡¨

        Returns:
            å¯ç”¨å®ä¾‹åˆ—è¡¨ï¼ŒæŒ‰å“åº”æ—¶é—´æ’åº
        """
        logger.info(f"å¼€å§‹æ£€æµ‹ {len(instances)} ä¸ªå®ä¾‹çš„å¥åº·çŠ¶æ€...")

        available_instances = []

        # å¹¶å‘æ£€æµ‹æ‰€æœ‰å®ä¾‹
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.check_instance, url): url
                for url in instances
            }

            for future in as_completed(futures):
                url, is_available, response_time = future.result()

                if is_available:
                    available_instances.append({
                        "url": url,
                        "response_time": response_time,
                        "status": "available"
                    })

        # æŒ‰å“åº”æ—¶é—´æ’åº
        available_instances.sort(key=lambda x: x["response_time"])

        logger.info(f"å‘ç° {len(available_instances)} ä¸ªå¯ç”¨å®ä¾‹")
        return available_instances


class NitterInstanceDiscovery:
    """Nitter å®ä¾‹å‘ç°ä¸ç®¡ç†ï¼ˆæ•´åˆæ¥æºè·å–å’Œå¥åº·æ£€æµ‹ï¼Œä½¿ç”¨ Redis ç¼“å­˜ï¼‰"""

    def __init__(self, timeout: int = 10, max_workers: int = 20):
        """
        åˆå§‹åŒ–å®ä¾‹å‘ç°å™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: å¹¶å‘æ£€æµ‹çº¿ç¨‹æ•°
        """
        self.checker = NitterInstanceChecker(timeout, max_workers)
        self.source_manager = get_default_sources()
        self.redis_client = None

    def _get_redis_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ– Redis å®¢æˆ·ç«¯"""
        if self.redis_client is None:
            try:
                self.redis_client = get_redis_client()
            except Exception as e:
                logger.warning(f"Redis è¿æ¥å¤±è´¥ï¼Œç¼“å­˜åŠŸèƒ½ä¸å¯ç”¨: {e}")
        return self.redis_client

    def _load_from_cache(self) -> Optional[List[Dict[str, any]]]:
        """
        ä» Redis ç¼“å­˜åŠ è½½å¯ç”¨å®ä¾‹

        Returns:
            ç¼“å­˜çš„å®ä¾‹åˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸè¿”å› None
        """
        redis = self._get_redis_client()
        if not redis:
            return None

        try:
            cached = redis.get_cache(REDIS_KEY_AVAILABLE_INSTANCES)
            if cached:
                instances = json.loads(cached)
                logger.info(f"âœ“ ä» Redis ç¼“å­˜åŠ è½½ {len(instances)} ä¸ªå¯ç”¨å®ä¾‹")
                return instances
            else:
                logger.debug("Redis ç¼“å­˜ä¸ºç©ºæˆ–å·²è¿‡æœŸ")
                return None
        except Exception as e:
            logger.warning(f"ä» Redis è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _save_to_cache(self, instances: List[Dict[str, any]]) -> bool:
        """
        ä¿å­˜å¯ç”¨å®ä¾‹åˆ° Redis ç¼“å­˜

        Args:
            instances: å¯ç”¨å®ä¾‹åˆ—è¡¨

        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        redis = self._get_redis_client()
        if not redis:
            return False

        try:
            data = json.dumps(instances, ensure_ascii=False)
            success = redis.set_cache(
                REDIS_KEY_AVAILABLE_INSTANCES,
                data,
                expire=CACHE_EXPIRE_INSTANCES
            )
            if success:
                logger.info(f"âœ“ å·²ä¿å­˜ {len(instances)} ä¸ªå¯ç”¨å®ä¾‹åˆ° Redis ç¼“å­˜ï¼ˆæœ‰æ•ˆæœŸ 3 å°æ—¶ï¼‰")
            return success
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ° Redis ç¼“å­˜å¤±è´¥: {e}")
            return False

    def get_available_instances(
        self,
        force_refresh: bool = False,
        use_external_sources: bool = True,
        min_instances: int = 5
    ) -> List[Dict[str, any]]:
        """
        è·å–å¯ç”¨å®ä¾‹åˆ—è¡¨ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼‰

        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            use_external_sources: æ˜¯å¦ä»ç¬¬ä¸‰æ–¹æ¥æºè·å–å®ä¾‹
            min_instances: æœ€å°‘è¿”å›çš„å®ä¾‹æ•°

        Returns:
            å¯ç”¨å®ä¾‹åˆ—è¡¨ï¼ŒæŒ‰å“åº”æ—¶é—´æ’åº
        """
        # å¦‚æœä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå…ˆå°è¯•ä»ç¼“å­˜è¯»å–
        if not force_refresh:
            cached = self._load_from_cache()
            if cached:
                return cached

        # ç¼“å­˜ä¸å­˜åœ¨æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œé‡æ–°å‘ç°å’Œæ£€æµ‹
        logger.info("å¼€å§‹é‡æ–°å‘ç°å’Œæ£€æµ‹å¯ç”¨å®ä¾‹...")
        instances = self.discover_available_instances(use_external_sources, min_instances)

        # ä¿å­˜åˆ°ç¼“å­˜
        if instances:
            self._save_to_cache(instances)

        return instances

    def discover_available_instances(
        self,
        use_external_sources: bool = True,
        min_instances: int = 5
    ) -> List[Dict[str, any]]:
        """
        å‘ç°æ‰€æœ‰å¯ç”¨çš„ Nitter å®ä¾‹

        Args:
            use_external_sources: æ˜¯å¦ä»ç¬¬ä¸‰æ–¹æ¥æºè·å–å®ä¾‹
            min_instances: æœ€å°‘è¿”å›çš„å®ä¾‹æ•°

        Returns:
            å¯ç”¨å®ä¾‹åˆ—è¡¨ï¼ŒæŒ‰å“åº”æ—¶é—´æ’åº
        """
        # åˆå¹¶é…ç½®çš„å®ä¾‹å’Œç¬¬ä¸‰æ–¹å®ä¾‹
        instances_to_check = set(KNOWN_INSTANCES)  # ä»é…ç½®å¼€å§‹
        logger.info(f"ä½¿ç”¨é…ç½®çš„ {len(KNOWN_INSTANCES)} ä¸ªå†…ç½®å®ä¾‹")

        # å¦‚æœå¯ç”¨ï¼Œä»ç¬¬ä¸‰æ–¹æ¥æºè·å–æ›´å¤šå®ä¾‹
        if use_external_sources:
            external_instances = self.source_manager.fetch_all_instances()
            instances_to_check.update(external_instances)
            logger.info(f"åˆå¹¶åå…± {len(instances_to_check)} ä¸ªå¾…æ£€æµ‹å®ä¾‹")
        else:
            logger.info("ä»…ä½¿ç”¨å†…ç½®å®ä¾‹ï¼Œè·³è¿‡ç¬¬ä¸‰æ–¹æ¥æº")

        # å¥åº·æ£€æµ‹
        available_instances = self.checker.check_instances_batch(list(instances_to_check))

        # å¦‚æœå¯ç”¨å®ä¾‹å¤ªå°‘ï¼Œè­¦å‘Š
        if len(available_instances) < min_instances:
            logger.warning(
                f"å¯ç”¨å®ä¾‹ä»… {len(available_instances)} ä¸ªï¼Œå°‘äºæœŸæœ›çš„ {min_instances} ä¸ª"
            )

        return available_instances

    def get_available_urls(
        self,
        max_count: int = 10,
        max_response_time: float = 5.0,
        force_refresh: bool = False
    ) -> List[str]:
        """
        è·å–å¯ç”¨å®ä¾‹ URL åˆ—è¡¨ï¼ˆä¼˜å…ˆä»ç¼“å­˜ï¼‰

        Args:
            max_count: æœ€å¤šè¿”å›çš„å®ä¾‹æ•°
            max_response_time: æœ€å¤§å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜

        Returns:
            URL åˆ—è¡¨ï¼ŒæŒ‰å“åº”æ—¶é—´æ’åº
        """
        instances = self.get_available_instances(force_refresh=force_refresh)

        # è¿‡æ»¤å“åº”æ—¶é—´
        filtered = [
            inst["url"] for inst in instances
            if inst.get("response_time", 0) <= max_response_time
        ]

        return filtered[:max_count]


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Nitter å®ä¾‹å‘ç°ä¸å¥åº·æ£€æµ‹ï¼ˆä½¿ç”¨ Redis ç¼“å­˜ï¼‰")
    parser.add_argument("--count", type=int, default=10, help="è¿”å›çš„æœ€å¤§å®ä¾‹æ•°")
    parser.add_argument("--timeout", type=int, default=10, help="è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-response-time", type=float, default=5.0, help="æœ€å¤§å“åº”æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--force-refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œé‡æ–°æ£€æµ‹")
    parser.add_argument("--no-external", action="store_true", help="ä¸ä»ç¬¬ä¸‰æ–¹æ¥æºè·å–ï¼Œåªä½¿ç”¨å†…ç½®åˆ—è¡¨")

    args = parser.parse_args()

    # åˆ›å»ºå‘ç°å™¨
    discovery = NitterInstanceDiscovery(timeout=args.timeout)

    # è·å–å¯ç”¨å®ä¾‹
    print("\n" + "=" * 80)
    print("Nitter å®ä¾‹å¥åº·æ£€æµ‹")
    print("=" * 80 + "\n")

    if args.force_refresh:
        print("âš¡ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå¿½ç•¥ç¼“å­˜\n")

    available_urls = discovery.get_available_urls(
        max_count=args.count,
        max_response_time=args.max_response_time,
        force_refresh=args.force_refresh
    )

    if available_urls:
        print(f"âœ“ å¯ç”¨å®ä¾‹ {len(available_urls)} ä¸ª:\n")
        for i, url in enumerate(available_urls, 1):
            print(f"  {i}. {url}")

        print("\nğŸ’¡ å®ä¾‹åˆ—è¡¨å·²ç¼“å­˜åˆ° Redisï¼ˆæœ‰æ•ˆæœŸ 3 å°æ—¶ï¼‰")
        print("   ä¸‹æ¬¡è°ƒç”¨å°†ç›´æ¥ä»ç¼“å­˜è¯»å–ï¼Œæ— éœ€é‡æ–°æ£€æµ‹")

    else:
        print("âœ— æœªå‘ç°å¯ç”¨å®ä¾‹")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()

