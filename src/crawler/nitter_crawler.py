"""
Nitter æ¨æ–‡çˆ¬è™«
è´Ÿè´£ä» Nitter å®ä¾‹è·å–ç”¨æˆ·æ¨æ–‡ä¿¡æ¯ï¼ˆä» Redis è·å–å¯ç”¨å®ä¾‹ï¼‰
"""

import time
import logging
from typing import List, Dict, Optional

from datetime import datetime, timezone

import requests
from bs4 import BeautifulSoup

from src.config.settings import settings
from src.crawler.constants import KNOWN_INSTANCES

logger = logging.getLogger(__name__)


class NitterCrawler:
    """Nitter æ¨æ–‡çˆ¬è™«ç±»"""

    def __init__(self, use_redis_instances: bool = True, max_instances: int = 20):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            use_redis_instances: æ˜¯å¦ä» Redis è·å–å®ä¾‹åˆ—è¡¨ï¼ˆæ¨èï¼‰
            max_instances: æœ€å¤šä½¿ç”¨çš„å®ä¾‹æ•°é‡ï¼ˆé»˜è®¤20ä¸ªï¼‰
        """
        self.use_redis_instances = use_redis_instances
        self.max_instances = max_instances
        self.timeout = settings.CRAWLER_TIMEOUT
        self.retry = settings.CRAWLER_RETRY
        self.delay = settings.CRAWLER_DELAY
        self.session = requests.Session()
        # ä½¿ç”¨æ›´å®Œæ•´çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
        })

        # åˆå§‹åŒ–å®ä¾‹åˆ—è¡¨
        self._instances_cache = []
        self._last_refresh = 0
        self._refresh_instances()

    def _refresh_instances(self):
        """åˆ·æ–°å®ä¾‹åˆ—è¡¨ï¼ˆè·å–å‰ N ä¸ªæœ€å¿«çš„å®ä¾‹ï¼‰"""
        current_time = time.time()

        # æ¯ 10 åˆ†é’Ÿæœ€å¤šåˆ·æ–°ä¸€æ¬¡
        if current_time - self._last_refresh < 600:
            return

        if self.use_redis_instances:
            # ä» Redis è·å–å¯ç”¨å®ä¾‹
            try:
                from src.crawler.instance_discovery import NitterInstanceDiscovery

                discovery = NitterInstanceDiscovery()
                instances_data = discovery.get_available_instances(force_refresh=False)

                if instances_data:
                    # æå– URLï¼Œå·²æŒ‰å“åº”æ—¶é—´æ’åºï¼Œå–å‰ N ä¸ª
                    self._instances_cache = [
                        inst["url"] for inst in instances_data[:self.max_instances]
                    ]
                    logger.info(
                        f"ä» Redis åŠ è½½å‰ {len(self._instances_cache)} ä¸ªå¯ç”¨å®ä¾‹"
                    )
                else:
                    logger.warning("Redis ä¸­æ²¡æœ‰å¯ç”¨å®ä¾‹ï¼Œä½¿ç”¨å†…ç½®åˆ—è¡¨")
                    self._instances_cache = KNOWN_INSTANCES[:self.max_instances].copy()

            except Exception as e:
                logger.error(f"ä» Redis è·å–å®ä¾‹å¤±è´¥: {e}ï¼Œä½¿ç”¨å†…ç½®åˆ—è¡¨")
                self._instances_cache = KNOWN_INSTANCES[:self.max_instances].copy()
        else:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å®ä¾‹
            self._instances_cache = settings.NITTER_INSTANCES[:self.max_instances]

        self._last_refresh = current_time

    def _request_with_retry(self, url: str, max_retries: int = None) -> Optional[requests.Response]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„ HTTP è¯·æ±‚

        Args:
            url: è¯·æ±‚ URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®

        Returns:
            Response å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        if max_retries is None:
            max_retries = settings.HTTP_RETRY_COUNT

        last_exception = None

        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(f"æ­£åœ¨è¯·æ±‚ {url} (å°è¯• {attempt}/{max_retries})")
                response = self.session.get(url, timeout=self.timeout, allow_redirects=True)
                response.raise_for_status()
                return response

            except requests.exceptions.Timeout as e:
                last_exception = e
                logger.debug(f"è¯·æ±‚è¶…æ—¶ {url} (å°è¯• {attempt}/{max_retries})")
                if attempt < max_retries:
                    time.sleep(settings.HTTP_RETRY_DELAY)
                    continue

            except requests.exceptions.ConnectionError as e:
                last_exception = e
                logger.debug(f"è¿æ¥é”™è¯¯ {url} (å°è¯• {attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    time.sleep(settings.HTTP_RETRY_DELAY)
                    continue

            except requests.exceptions.HTTPError as e:
                # HTTP é”™è¯¯ï¼ˆ4xx, 5xxï¼‰é€šå¸¸ä¸åº”è¯¥é‡è¯•ï¼Œé™¤éæ˜¯ 5xx æœåŠ¡å™¨é”™è¯¯
                last_exception = e
                if e.response.status_code >= 500:
                    logger.debug(f"æœåŠ¡å™¨é”™è¯¯ {url} (å°è¯• {attempt}/{max_retries}): {e.response.status_code}")
                    if attempt < max_retries:
                        time.sleep(settings.HTTP_RETRY_DELAY)
                        continue
                # 4xx å®¢æˆ·ç«¯é”™è¯¯ä¸é‡è¯•
                logger.debug(f"HTTP é”™è¯¯ {url}: {e.response.status_code}")
                return None

            except requests.RequestException as e:
                last_exception = e
                logger.debug(f"è¯·æ±‚å¼‚å¸¸ {url} (å°è¯• {attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    time.sleep(settings.HTTP_RETRY_DELAY)
                    continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.warning(f"è¯·æ±‚å¤±è´¥ {url}ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {last_exception}")
        return None

    def fetch_user_timeline(
        self, username: str, max_tweets: int = 20
    ) -> Optional[List[Dict]]:
        """
        è·å–ç”¨æˆ·æ—¶é—´çº¿æ¨æ–‡ï¼ˆæŒ‰é¡ºåºå°è¯•æ‰€æœ‰å¯ç”¨å®ä¾‹ï¼‰

        Args:
            username: Twitter ç”¨æˆ·å
            max_tweets: æœ€å¤§è·å–æ¨æ–‡æ•°é‡

        Returns:
            æ¨æ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ¨æ–‡ä¸ºå­—å…¸æ ¼å¼
            å¦‚æœæ‰€æœ‰å®ä¾‹éƒ½å¤±è´¥ï¼Œè¿”å› None
            å¦‚æœæˆåŠŸä½†æ²¡æœ‰æ¨æ–‡ï¼Œè¿”å›ç©ºåˆ—è¡¨ []
        """
        # åˆ·æ–°å®ä¾‹åˆ—è¡¨
        self._refresh_instances()

        if not self._instances_cache:
            logger.error("æ²¡æœ‰å¯ç”¨çš„ Nitter å®ä¾‹")
            return None

        tweets = []

        # æŒ‰é¡ºåºå°è¯•æ¯ä¸ªå®ä¾‹
        for instance_index, instance in enumerate(self._instances_cache, 1):
            try:
                url = f"{instance}/{username}"

                logger.info(
                    f"å°è¯•ä»å®ä¾‹ {instance_index}/{len(self._instances_cache)}: "
                    f"{instance} è·å–ç”¨æˆ· {username} çš„æ¨æ–‡"
                )

                # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚æ–¹æ³•
                response = self._request_with_retry(url)

                if not response:
                    logger.warning(f"âœ— {instance} è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹")
                    if instance_index < len(self._instances_cache):
                        time.sleep(self.delay)
                    continue

                # ä¿å­˜å½“å‰å®ä¾‹URLï¼ˆç”¨äºè¡¥å…¨åª’ä½“ç›¸å¯¹è·¯å¾„ï¼‰
                self.current_instance = instance

                # è§£æ HTML
                tweets = self._parse_timeline(response.text, username)

                if tweets:
                    logger.info(
                        f"âœ“ æˆåŠŸä» {instance} è·å– {len(tweets)} æ¡æ¨æ–‡"
                    )
                    return tweets[:max_tweets]
                else:
                    logger.warning(f"âœ— {instance} è¿”å›ç©ºæ¨æ–‡åˆ—è¡¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹")
                    continue

            except Exception as e:
                logger.error(f"âœ— å®ä¾‹ {instance} è§£æå¤±è´¥: {e}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹")
                if instance_index < len(self._instances_cache):
                    time.sleep(self.delay)
                continue

        # æ‰€æœ‰å®ä¾‹éƒ½å¤±è´¥
        logger.error(
            f"æ‰€æœ‰ {len(self._instances_cache)} ä¸ªå®ä¾‹éƒ½æ— æ³•è·å–ç”¨æˆ· {username} çš„æ¨æ–‡"
        )
        return None

    def _parse_timeline(self, html: str, username: str) -> List[Dict]:
        """
        è§£ææ—¶é—´çº¿ HTMLï¼Œæå–æ¨æ–‡ä¿¡æ¯

        Args:
            html: HTML å†…å®¹
            username: ç”¨æˆ·å

        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        soup = BeautifulSoup(html, "html.parser")
        tweets = []

        # æŸ¥æ‰¾æ‰€æœ‰æ¨æ–‡å®¹å™¨
        tweet_items = soup.find_all("div", class_="timeline-item")

        for item in tweet_items:
            try:
                tweet_data = self._extract_tweet_data(item, username)
                if tweet_data:
                    tweets.append(tweet_data)
            except Exception as e:
                logger.warning(f"è§£æå•æ¡æ¨æ–‡å¤±è´¥: {e}")
                continue

        return tweets

    def _extract_tweet_data(self, item, username: str) -> Optional[Dict]:
        """
        ä»æ¨æ–‡å…ƒç´ ä¸­æå–æ•°æ®

        Args:
            item: BeautifulSoup æ¨æ–‡å…ƒç´ 
            username: ç”¨æˆ·å

        Returns:
            æ¨æ–‡æ•°æ®å­—å…¸
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç½®é¡¶æ¨æ–‡
        is_pinned = item.find("div", class_="pinned") is not None

        # æå–æ¨æ–‡é“¾æ¥å’Œ ID
        link = item.find("a", class_="tweet-link")
        if not link:
            return None

        tweet_url = link.get("href", "")
        # tweet_id é€šå¸¸åœ¨ URL æœ€å: /username/status/1234567890
        parts = tweet_url.split("/")
        if len(parts) < 4 or parts[-2] != "status":
            return None

        tweet_id = parts[-1].split("#")[0]  # å»é™¤å¯èƒ½çš„é”šç‚¹

        # æå–æ¨æ–‡å†…å®¹
        content_div = item.find("div", class_="tweet-content")
        content = content_div.get_text(strip=True) if content_div else ""

        # æå–å‘å¸ƒæ—¶é—´
        time_span = item.find("span", class_="tweet-date")
        published_at = self._parse_timestamp(time_span) if time_span else datetime.now(timezone.utc)

        # æå–ä½œè€…ä¿¡æ¯
        author_div = item.find("a", class_="username")
        author = author_div.get_text(strip=True) if author_div else username

        # æå–åª’ä½“ä¿¡æ¯ï¼ˆå›¾ç‰‡ã€è§†é¢‘ã€GIFï¼‰
        media_urls = self._extract_media_urls(item)

        # æ„å»º x.com åŸå§‹é“¾æ¥ï¼ˆç”¨äºæº¯æºï¼‰
        twitter_url = f"https://x.com/{author.lstrip('@')}/status/{tweet_id}"

        return {
            "tweet_id": tweet_id,
            "author": author.lstrip("@"),
            "author_id": "",  # Nitter é€šå¸¸ä¸æä¾› author_id
            "content": content,
            "published_at": published_at,
            "tweet_url": twitter_url,  # ä¿å­˜ x.com åŸå§‹é“¾æ¥
            "media_urls": media_urls,  # åª’ä½“URLåˆ—è¡¨
            "is_pinned": is_pinned,  # æ ‡è®°æ˜¯å¦ä¸ºç½®é¡¶æ¨æ–‡
        }

    def _extract_media_urls(self, item) -> List[str]:
        """
        ä»æ¨æ–‡å…ƒç´ ä¸­æå–åª’ä½“URL

        Args:
            item: BeautifulSoup æ¨æ–‡å…ƒç´ 

        Returns:
            åª’ä½“URLåˆ—è¡¨
        """
        media_urls = []

        try:
            # æŸ¥æ‰¾åª’ä½“é™„ä»¶å®¹å™¨
            attachments = item.find("div", class_="attachments")
            if not attachments:
                return media_urls

            # æå–å›¾ç‰‡
            images = attachments.find_all("a", class_="still-image")
            for img_link in images:
                # è·å–é«˜æ¸…å›¾ç‰‡é“¾æ¥
                img = img_link.find("img")
                if img:
                    # å°è¯•è·å–åŸå›¾é“¾æ¥
                    src = img.get("src", "")
                    if src:
                        # Nitter å›¾ç‰‡é€šå¸¸æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦è¡¥å…¨
                        if src.startswith("/"):
                            # ä½¿ç”¨å½“å‰å®ä¾‹çš„åŸŸå
                            if hasattr(self, 'current_instance'):
                                src = f"{self.current_instance}{src}"
                        media_urls.append(src)

            # æå–è§†é¢‘/GIF
            videos = attachments.find_all("video")
            for video in videos:
                source = video.find("source")
                if source:
                    src = source.get("src", "")
                    if src:
                        if src.startswith("/"):
                            if hasattr(self, 'current_instance'):
                                src = f"{self.current_instance}{src}"
                        media_urls.append(src)

            # æå– GIFï¼ˆæœ‰æ—¶åœ¨å•ç‹¬çš„å®¹å™¨ä¸­ï¼‰
            gif_containers = attachments.find_all("div", class_="gif")
            for gif_div in gif_containers:
                video = gif_div.find("video")
                if video:
                    source = video.find("source")
                    if source:
                        src = source.get("src", "")
                        if src:
                            if src.startswith("/"):
                                if hasattr(self, 'current_instance'):
                                    src = f"{self.current_instance}{src}"
                            media_urls.append(src)

        except Exception as e:
            logger.debug(f"æå–åª’ä½“URLå¤±è´¥: {e}")

        return media_urls

    def _parse_timestamp(self, time_element) -> datetime:
        """
        è§£ææ—¶é—´æˆ³

        Args:
            time_element: æ—¶é—´å…ƒç´ 

        Returns:
            datetime å¯¹è±¡
        """
        try:
            # æ–¹æ³•1: ä» a æ ‡ç­¾çš„ title å±æ€§è·å–å®Œæ•´æ—¶é—´
            link = time_element.find("a")
            if link:
                title = link.get("title", "")
                if title:
                    # æ ¼å¼: "Dec 22, 2025 Â· 5:47 AM UTC"
                    # åˆ†å‰²å¹¶è§£æ
                    parts = title.split("Â·")
                    if len(parts) >= 2:
                        date_part = parts[0].strip()  # "Dec 22, 2025"
                        time_part = parts[1].strip()  # "5:47 AM UTC"

                        # ç»„åˆå®Œæ•´æ—¶é—´å­—ç¬¦ä¸²
                        datetime_str = f"{date_part} {time_part}"

                        # å°è¯•å¤šç§æ ¼å¼è§£æ
                        formats = [
                            "%b %d, %Y %I:%M %p UTC",  # Dec 22, 2025 5:47 AM UTC
                            "%b %d, %Y %H:%M UTC",      # Dec 22, 2025 17:47 UTC
                        ]

                        for fmt in formats:
                            try:
                                # è§£ææ—¶é—´å¹¶æ·»åŠ UTCæ—¶åŒºä¿¡æ¯
                                dt = datetime.strptime(datetime_str, fmt)
                                return dt.replace(tzinfo=timezone.utc)
                            except ValueError:
                                continue

            # æ–¹æ³•2: ä» span çš„ title å±æ€§ç›´æ¥è·å–
            title = time_element.get("title", "")
            if title and "Â·" in title:
                parts = title.split("Â·")
                date_part = parts[0].strip()
                time_part = parts[1].strip()
                datetime_str = f"{date_part} {time_part}"

                formats = [
                    "%b %d, %Y %I:%M %p UTC",
                    "%b %d, %Y %H:%M UTC",
                ]

                for fmt in formats:
                    try:
                        # è§£ææ—¶é—´å¹¶æ·»åŠ UTCæ—¶åŒºä¿¡æ¯
                        dt = datetime.strptime(datetime_str, fmt)
                        return dt.replace(tzinfo=timezone.utc)
                    except ValueError:
                        continue

        except Exception as e:
            logger.debug(f"è§£ææ—¶é—´æˆ³å¤±è´¥: {e}")

        # é»˜è®¤è¿”å›å½“å‰UTCæ—¶é—´
        logger.warning("æ— æ³•è§£ææ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰UTCæ—¶é—´")
        return datetime.now(timezone.utc)


if __name__ == "__main__":
    """è°ƒè¯•å…¥å£"""
    import sys
    from bs4 import BeautifulSoup

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(
        level=logging.DEBUG,
        format="[%(asctime)s] [%(levelname)s] [%(name)s] - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    print("=" * 80)
    print("Nitter çˆ¬è™«è°ƒè¯•æµ‹è¯•")
    print("=" * 80)
    print()

    # æµ‹è¯•å‚æ•°
    test_instance = "https://nitter.tiekoetter.com"
    test_username = "by0x_1993"
    test_url = f"{test_instance}/{test_username}"

    print(f"æµ‹è¯•å®ä¾‹: {test_instance}")
    print(f"æµ‹è¯•ç”¨æˆ·: {test_username}")
    print(f"æµ‹è¯• URL: {test_url}")
    print()

    # åˆå§‹åŒ–çˆ¬è™«
    crawler = NitterCrawler(use_redis_instances=False, max_instances=1)

    print("ã€æµ‹è¯• 1ã€‘ç›´æ¥è®¿é—®ç”¨æˆ·é¡µé¢")
    print("-" * 80)

    try:
        # ç›´æ¥è®¿é—®æµ‹è¯•
        print("\n1. å‘é€ HTTP è¯·æ±‚...")
        response = crawler._request_with_retry(test_url)

        if not response:
            print("   âœ— è¯·æ±‚å¤±è´¥")
            sys.exit(1)

        print(f"   âœ“ è¯·æ±‚æˆåŠŸ")
        print(f"   çŠ¶æ€ç : {response.status_code}")
        print(f"   å®é™… URL: {response.url}")
        print(f"   å“åº”ç¼–ç : {response.encoding}")
        print(f"   Content-Encoding: {response.headers.get('Content-Encoding', 'None')}")

        content = response.text
        print(f"   å“åº”å¤§å°: {len(content)} å­—ç¬¦")
        print(f"   å“åº”å­—èŠ‚å¤§å°: {len(response.content)} å­—èŠ‚")

        # ä¿å­˜å“åº”å†…å®¹
        debug_file = "/tmp/nitter_user_page_debug.html"
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"   å“åº”å†…å®¹å·²ä¿å­˜åˆ°: {debug_file}")

        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        print(f"\n2. å“åº”å†…å®¹é¢„è§ˆï¼ˆå‰ 500 å­—ç¬¦ï¼‰:")
        print("   " + "-" * 76)
        preview = content[:500].replace("\n", "\n   ")
        print(f"   {preview}")
        print("   " + "-" * 76)

        # HTML ç»“æ„æ£€æŸ¥
        print(f"\n3. HTML ç»“æ„æ£€æŸ¥:")
        soup = BeautifulSoup(content, "html.parser")

        timeline_items = soup.find_all("div", class_="timeline-item")
        print(f"   - æ‰¾åˆ° timeline-item: {len(timeline_items)} ä¸ª")

        # æ£€æŸ¥ç½®é¡¶æ¨æ–‡
        pinned_items = [item for item in timeline_items if item.find("div", class_="pinned")]
        if pinned_items:
            print(f"   - æ£€æµ‹åˆ° {len(pinned_items)} æ¡ç½®é¡¶æ¨æ–‡ ğŸ“Œ")

        if timeline_items:
            first_item = timeline_items[0]
            is_pinned = first_item.find("div", class_="pinned") is not None
            pinned_text = " (ç½®é¡¶æ¨æ–‡ ğŸ“Œ)" if is_pinned else ""
            print(f"\n   ç¬¬ä¸€ä¸ª timeline-item çš„ç»“æ„{pinned_text}:")
            print(f"   - åŒ…å« tweet-link: {'æ˜¯' if first_item.find('a', class_='tweet-link') else 'å¦'}")
            print(f"   - åŒ…å« tweet-content: {'æ˜¯' if first_item.find('div', class_='tweet-content') else 'å¦'}")
            print(f"   - åŒ…å« tweet-date: {'æ˜¯' if first_item.find('span', class_='tweet-date') else 'å¦'}")
            print(f"   - åŒ…å« username: {'æ˜¯' if first_item.find('a', class_='username') else 'å¦'}")
            print(f"   - æ˜¯å¦ç½®é¡¶: {'æ˜¯' if is_pinned else 'å¦'}")

            # æ˜¾ç¤ºç¬¬ä¸€ä¸ª item çš„ HTML
            print(f"\n   ç¬¬ä¸€ä¸ª timeline-item çš„ HTMLï¼ˆå‰ 300 å­—ç¬¦ï¼‰:")
            print("   " + "-" * 76)
            item_html = str(first_item)[:300].replace("\n", "\n   ")
            print(f"   {item_html}")
            print("   " + "-" * 76)

        # æ£€æŸ¥å…¶ä»–å…³é”®å…ƒç´ 
        print(f"\n4. é¡µé¢å…ƒç´ æ£€æŸ¥:")
        print(f"   - åŒ…å« <html>: {'æ˜¯' if '<html' in content.lower() else 'å¦'}")
        print(f"   - åŒ…å« <body>: {'æ˜¯' if '<body' in content.lower() else 'å¦'}")
        print(f"   - åŒ…å« 'timeline': {'æ˜¯' if 'timeline' in content.lower() else 'å¦'}")
        print(f"   - åŒ…å« 'tweet': {'æ˜¯' if 'tweet' in content.lower() else 'å¦'}")
        print(f"   - åŒ…å«ç”¨æˆ·å '{test_username}': {'æ˜¯' if test_username.lower() in content.lower() else 'å¦'}")

    except Exception as e:
        print(f"   âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    print("\n" + "=" * 80)
    print("ã€æµ‹è¯• 2ã€‘ä½¿ç”¨ fetch_user_timeline æ–¹æ³•è·å–æ¨æ–‡")
    print("-" * 80)

    try:
        # ä¸´æ—¶è®¾ç½®å®ä¾‹ç¼“å­˜ä¸ºæµ‹è¯•å®ä¾‹
        crawler._instances_cache = [test_instance]

        tweets = crawler.fetch_user_timeline(test_username, max_tweets=5)

        if tweets is None:
            print("\nâœ— è·å–å¤±è´¥ï¼šæ‰€æœ‰å®ä¾‹éƒ½å¤±è´¥")
            sys.exit(1)
        elif not tweets:
            print("\nâœ“ è·å–æˆåŠŸï¼Œä½†æ²¡æœ‰æ‰¾åˆ°æ¨æ–‡")
            print("\nå¯èƒ½åŸå› ï¼š")
            print("  1. HTML ç»“æ„ä¸é¢„æœŸä¸ç¬¦ï¼ˆé€‰æ‹©å™¨ä¸åŒ¹é…ï¼‰")
            print("  2. é¡µé¢ä½¿ç”¨ JavaScript åŠ¨æ€åŠ è½½å†…å®¹")
            print("  3. ç”¨æˆ·æ²¡æœ‰æ¨æ–‡æˆ–æ¨æ–‡è¢«éšè—")
            print(f"\nè¯·æ£€æŸ¥ä¿å­˜çš„ HTML æ–‡ä»¶: {debug_file}")
        else:
            print(f"\nâœ“ æˆåŠŸè·å–åˆ° {len(tweets)} æ¡æ¨æ–‡:\n")
            for i, tweet in enumerate(tweets, 1):
                pinned_mark = " [ğŸ“Œ ç½®é¡¶]" if tweet.get("is_pinned", False) else ""
                print(f"ã€æ¨æ–‡ {i}ã€‘{pinned_mark}")
                print(f"  Tweet ID: {tweet['tweet_id']}")
                print(f"  Author: {tweet['author']}")
                print(f"  Content: {tweet['content'][:150]}{'...' if len(tweet['content']) > 150 else ''}")
                print(f"  Published: {tweet['published_at']}")
                print(f"  URL: {tweet['tweet_url']}")
                print()

    except Exception as e:
        print(f"\nâœ— è·å–æ¨æ–‡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("=" * 80)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 80)

